{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pdb\n",
    "from scipy.special import expit\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\" Softmax function \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the softmax function for the input here.                #\n",
    "    # It is crucial that this function is optimized for speed because #\n",
    "    # it will be used frequently in later code.                       #\n",
    "    # You might find numpy functions np.exp, np.sum, np.reshape,      #\n",
    "    # np.max, and numpy broadcasting useful for this task. (numpy     #\n",
    "    # broadcasting documentation:                                     #\n",
    "    # http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)  #\n",
    "    # You should also make sure that your code works for one          #\n",
    "    # dimensional inputs (treat the vector as a row), you might find  #\n",
    "    # it helpful for your later problems.                             #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    if len(np.shape(x)) <= 1:\n",
    "        x = x - np.max(x)\n",
    "        ex = np.exp(x)\n",
    "        return ex / np.sum(ex)\n",
    "    x = x - np.max(x, -1)[:, np.newaxis]\n",
    "    ex = np.exp(x)\n",
    "    \n",
    "    return ex / np.sum(ex, -1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\" Sigmoid function \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the sigmoid function for the input here.                #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    MIN = 10e-7\n",
    "    s = expit(x)\n",
    "    s[s < MIN] = MIN\n",
    "    s[s > 1 - MIN] = 1 - MIN\n",
    "    \n",
    "    return s\n",
    "\n",
    "def sigmoid_grad(f):\n",
    "    \"\"\" Sigmoid gradient function \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the gradient for the sigmoid function here. Note that   #\n",
    "    # for this implementation, the input f should be the sigmoid      #\n",
    "    # function value of your original input x.                        #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def gradcheck_naive(f, x):\n",
    "    \"\"\" \n",
    "    Gradient check for a function f \n",
    "    - f should be a function that takes a single argument and outputs the cost and its gradients\n",
    "    - x is the point (numpy array) to check the gradient at\n",
    "    \"\"\" \n",
    "\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)  \n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4\n",
    "\n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "    \n",
    "        ### YOUR CODE HERE: try modifying x[ix] with h defined above to compute numerical gradients\n",
    "        ### make sure you call random.setstate(rndstate) before calling f(x) each time, this will make it \n",
    "        ### possible to test cost functions with built in randomness later\n",
    "    \n",
    "        x2 = np.array(x)\n",
    "        x2[ix] += h\n",
    "        random.setstate(rndstate)\n",
    "        fx2, _ = f(x2)\n",
    "\n",
    "        x1 = np.array(x)\n",
    "        x1[ix] -= h\n",
    "        random.setstate(rndstate)\n",
    "        fx1, _ = f(x1)\n",
    "\n",
    "        numgrad = (fx2-fx1)/(2*h)\n",
    "        ### END YOUR CODE\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        #print \"the numgrad is %f and the the grad is %f\" % (numgrad,grad[ix])\n",
    "        if reldiff > 1e-5:\n",
    "            print \"Gradient check failed.\"\n",
    "            print \"First gradient error found at index %s\" % str(ix)\n",
    "            print \"Your gradient: %f \\t Numerical gradient: %f\" % (grad[ix], numgrad)\n",
    "            #return\n",
    "    \n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print \"Gradient check passed!\"\n",
    "\n",
    "# Implement a function that normalizes each row of a matrix to have unit length\n",
    "def normalizeRows(x):\n",
    "    \"\"\" Row normalization function \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    x2 = x*x\n",
    "    ### END YOUR CODE\n",
    "    if len(np.shape(x)) <= 1:\n",
    "        return np.sqrt(x2 / np.sum(x2))\n",
    "    \n",
    "    return np.sqrt(x2/(np.sum(x2, -1)[:, np.newaxis]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print softmax(np.array([[1001,1002],[3,4]]))\n",
    "    print softmax(np.array([[-1001,-1002]]))\n",
    "    x = np.array([[1, 2], [-1, -2]])\n",
    "    f = sigmoid(x)\n",
    "    g = sigmoid_grad(f)\n",
    "    print \"=== For autograder ===\"\n",
    "    print f\n",
    "    print g\n",
    "    # First implement a gradient checker by filling in the following functions\n",
    "\n",
    "    # Sanity check for the gradient checker\n",
    "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "    print \"=== For autograder ===\"\n",
    "    gradcheck_naive(quad, np.array(123.456))      # scalar test\n",
    "    gradcheck_naive(quad, np.random.randn(3,))    # 1-D test\n",
    "    gradcheck_naive(quad, np.random.randn(4,5))   # 2-D test\n",
    "\n",
    "    # Test this function\n",
    "    print \"=== For autograder ===\"\n",
    "    print normalizeRows(np.array([[3.0,4.0],[1, 2]]))  # the result should be [[0.6, 0.8], [0.4472, 0.8944]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs224d.data_utils import *\n",
    "from scipy.special import expit\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Implement your skip-gram and CBOW models here\n",
    "\n",
    "# Interface to the dataset for negative sampling\n",
    "dataset = type('dummy', (), {})()\n",
    "def dummySampleTokenIdx():\n",
    "    return random.randint(0, 4)\n",
    "def getRandomContext(C):\n",
    "    tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "    return tokens[random.randint(0,4)], [tokens[random.randint(0,4)] for i in xrange(2*C)]\n",
    "dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "dataset.getRandomContext = getRandomContext\n",
    "\n",
    "def softmaxCostAndGradient(predicted, target, outputVectors):\n",
    "    \"\"\" Softmax cost function for word2vec models \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement the cost and gradients for one predicted word vector  #\n",
    "    # and one target word vector as a building block for word2vec     #\n",
    "    # models, assuming the softmax prediction function and cross      #\n",
    "    # entropy loss.                                                   #\n",
    "    # Inputs:                                                         #\n",
    "    #   - predicted: numpy ndarray, predicted word vector (\\hat{r} in #\n",
    "    #           the written component)                                #\n",
    "    #   - target: integer, the index of the target word               #\n",
    "    #   - outputVectors: \"output\" vectors for all tokens              #\n",
    "    # Outputs:                                                        #\n",
    "    #   - cost: cross entropy cost for the softmax word prediction    #\n",
    "    #   - gradPred: the gradient with respect to the predicted word   #\n",
    "    #           vector                                                #\n",
    "    #   - grad: the gradient with respect to all the other word       # \n",
    "    #           vectors                                               #\n",
    "    # We will not provide starter code for this function, but feel    #\n",
    "    # free to reference the code you previously wrote for this        #\n",
    "    # assignment!                                                     #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    # predicted 1xD\n",
    "    # target 1x1\n",
    "    # outputVectors VxD\n",
    "    \n",
    "    # cost 1x1\n",
    "    # gradPred 1xD\n",
    "    # grad VxD\n",
    "    D = len(predicted) \n",
    "\n",
    "    r_W = np.dot(predicted, outputVectors.T) # 1xV\n",
    "    r_W_soft = softmax(r_W) # 1xV\n",
    "    cost = -np.log(r_W_soft[target]) # 1x1\n",
    "    \n",
    "    gradPred = -outputVectors[target,:] + np.dot(r_W_soft, outputVectors)\n",
    "    grad = np.tile(r_W_soft, (D, 1)).T * predicted\n",
    "    grad[target,:] -= predicted\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradPred, grad\n",
    "\n",
    "def negSamplingCostAndGradient(predicted, target, outputVectors, K=10):\n",
    "    \"\"\" Negative sampling cost function for word2vec models \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement the cost and gradients for one predicted word vector  #\n",
    "    # and one target word vector as a building block for word2vec     #\n",
    "    # models, using the negative sampling technique. K is the sample  #\n",
    "    # size. You might want to use dataset.sampleTokenIdx() to sample  #\n",
    "    # a random word index.                                            #\n",
    "    # Input/Output Specifications: same as softmaxCostAndGradient     #\n",
    "    # We will not provide starter code for this function, but feel    #\n",
    "    # free to reference the code you previously wrote for this        #\n",
    "    # assignment!                                                     #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    # predicted 1xD\n",
    "    # target 1x1\n",
    "    # outputVectors VxD\n",
    "    \n",
    "    # cost 1x1\n",
    "    # gradPred 1xD\n",
    "    # grad VxD\n",
    "\n",
    "    negative_samples = [dataset.sampleTokenIdx() for i in range(K)]\n",
    "\n",
    "    r_W = np.dot(predicted, outputVectors.T) # 1xV\n",
    "    r_W_prob = sigmoid(r_W) # 1xV\n",
    "\n",
    "    cost = -np.log(r_W_prob[target]) -np.sum(np.log(1 - r_W_prob[negative_samples]))\n",
    "    \n",
    "    gradPred = -outputVectors[target,:] * (1 - r_W_prob[target])\n",
    "    gradPred += np.dot(r_W_prob[negative_samples], outputVectors[negative_samples, :])\n",
    "\n",
    "    grad = np.zeros(np.shape(outputVectors))\n",
    "    grad[target, :] = -predicted * (1-r_W_prob[target])\n",
    "\n",
    "    for negative_sample in negative_samples:\n",
    "        grad[negative_sample,:] += predicted*r_W_prob[negative_sample]\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradPred, grad\n",
    "\n",
    "def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement the skip-gram model in this function.                 #         \n",
    "    # Inputs:                                                         #\n",
    "    #   - currrentWord: a string of the current center word           #\n",
    "    #   - C: integer, context size                                    #\n",
    "    #   - contextWords: list of no more than 2*C strings, the context #\n",
    "    #             words                                               #\n",
    "    #   - tokens: a dictionary that maps words to their indices in    #\n",
    "    #             the word vector list                                #\n",
    "    #   - inputVectors: \"input\" word vectors for all tokens           #\n",
    "    #   - outputVectors: \"output\" word vectors for all tokens         #\n",
    "    #   - word2vecCostAndGradient: the cost and gradient function for #\n",
    "    #             a prediction vector given the target word vectors,  #\n",
    "    #             could be one of the two cost functions you          #\n",
    "    #             implemented above                                   #\n",
    "    # Outputs:                                                        #\n",
    "    #   - cost: the cost function value for the skip-gram model       #\n",
    "    #   - grad: the gradient with respect to the word vectors         #\n",
    "    # We will not provide starter code for this function, but feel    #\n",
    "    # free to reference the code you previously wrote for this        #\n",
    "    # assignment!                                                     #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    # inputVectors VxD\n",
    "    # outputVectors VxD\n",
    "\n",
    "    # cost float\n",
    "    # gradIn VxD\n",
    "    # gradOut VxD\n",
    "    cost = 0\n",
    "    predicted = inputVectors[tokens[currentWord], :] \n",
    "    gradIn = np.zeros(np.shape(inputVectors))\n",
    "    gradOut = np.zeros(np.shape(outputVectors))\n",
    "    for c_word in contextWords:\n",
    "        target = tokens[c_word]\n",
    "        c_cost, c_gradPred, c_grad = word2vecCostAndGradient(predicted, target, outputVectors)\n",
    "        cost += c_cost\n",
    "\n",
    "        gradIn[tokens[currentWord],:] += c_gradPred\n",
    "        gradOut += c_grad\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradIn, gradOut\n",
    "\n",
    "def cbow(currentWord, C, contextWords, tokens, inputVectors, outputVectors, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    \"\"\" CBOW model in word2vec \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement the continuous bag-of-words model in this function.   #         \n",
    "    # Input/Output specifications: same as the skip-gram model        #\n",
    "    # We will not provide starter code for this function, but feel    #\n",
    "    # free to reference the code you previously wrote for this        #\n",
    "    # assignment!                                                     #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    cost = 0\n",
    "    gradIn =0\n",
    "    gradOut =0\n",
    "    return cost, gradIn, gradOut\n",
    "\n",
    "# Gradient check!\n",
    "\n",
    "def word2vec_sgd_wrapper(word2vecModel, tokens, wordVectors, dataset, C, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    batchsize = 50\n",
    "    cost = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    inputVectors = wordVectors[:N/2,:]\n",
    "    outputVectors = wordVectors[N/2:,:]\n",
    "    for i in xrange(batchsize):\n",
    "        C1 = random.randint(1,C)\n",
    "        centerword, context = dataset.getRandomContext(C1)\n",
    "        #print \"center:\", centerword\n",
    "        #print \"context:\", context\n",
    "        \n",
    "        if word2vecModel == skipgram:\n",
    "            denom = 1\n",
    "        else:\n",
    "            denom = 1\n",
    "        \n",
    "        c, gin, gout = word2vecModel(centerword, C1, context, tokens, inputVectors, outputVectors, word2vecCostAndGradient)\n",
    "        cost += c / batchsize / denom\n",
    "        grad[:N/2, :] += gin / batchsize / denom\n",
    "        grad[N/2:, :] += gout / batchsize / denom\n",
    "        \n",
    "    return cost, grad\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(314159)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.rand(10,3))\n",
    "    dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "    print \"==== Gradient check for skip-gram ====\"\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(skipgram, dummy_tokens, vec, dataset, 5), dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient), dummy_vectors)\n",
    "    print \"\\n==== Gradient check for CBOW      ====\"\n",
    "    #gradcheck_naive(lambda vec: word2vec_sgd_wrapper(cbow, dummy_tokens, vec, dataset, 5), dummy_vectors)\n",
    "    #gradcheck_naive(lambda vec: word2vec_sgd_wrapper(cbow, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient), dummy_vectors)\n",
    "\n",
    "    print \"\\n=== For autograder ===\"\n",
    "    #print skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:])\n",
    "    #print skipgram(\"c\", 1, [\"a\", \"b\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], negSamplingCostAndGradient)\n",
    "    #print cbow(\"a\", 2, [\"a\", \"b\", \"c\", \"a\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:])\n",
    "    #print cbow(\"a\", 2, [\"a\", \"b\", \"a\", \"c\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], negSamplingCostAndGradient)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
